{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1bslYAz5kSdbjxBr5EtOXz6lbOhhf3nii",
      "authorship_tag": "ABX9TyOusPA052Tj0cjxUfjSOQKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaketMunda/mnist-digit-recognition/blob/master/recognise_handwritten_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digit Recognition\n",
        "\n",
        "Deep Learning model for Handwritten Digit Recognition using TensorFlow and Neural Network Techniques\n",
        "\n",
        "## 1. Problem Definition â›‘\n",
        "\n",
        "Recognise handwritten digit, from a dataset which contains B&W images of each digit written on 28x28 pixel box.\n",
        "\n",
        "## 2. Data \n",
        "\n",
        "The data we're using is officially provided by [The MNIST DATABASE](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "The digits have been sized-normalized and centered in a fixed-sized image.\n",
        "\n",
        "The data is quite preprocessed and well-formatted.\n",
        "\n",
        "## 3. Evaluation\n",
        "\n",
        "Accuracy should be above 90%.\n",
        "\n",
        "## 4. Features\n",
        "\n",
        "Some information about the data,\n",
        "* We're dealing with images(unstructured data) so it's probably best we use deep learning/transfer learning technique to solve this problem.\n",
        "* There are around a 60,000 examples of training set. \n",
        "* There are around a 10,000 examples of test set."
      ],
      "metadata": {
        "id": "O7o-vJCUoMr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Workspace ready !\n",
        "\n",
        "* Import Tensorflow\n",
        "* Import standard libraries to be used in this experiment"
      ],
      "metadata": {
        "id": "oAfdV77pFsDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "_BIXOY5lq_NE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573eac32-fd96-4f3d-e181-79021a89d822"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard helper libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7Yern88eF5dX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting our data ready\n",
        "\n",
        "The same MNIST dataset is also available in TensorFlow datasets. Thankfully it's splitted into training and test set which enable us to deep dive directly into exploring and visualizing the data.\n",
        "\n",
        "### Import the data\n",
        "\n",
        "Let's import the data using `tensorflow.keras.datasets` library"
      ],
      "metadata": {
        "id": "5BNIeOKtHem7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# The data is splitted into train and test set\n",
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "iKgbFwzSIyIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b46bf21-4837-4278-d08b-7b154565819b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the data\n",
        "\n",
        "Before going further and jumping directly to any step, exploring the data will help us to decide what are the things we need to do with our data,\n",
        "* Find outliers\n",
        "* If we need a preprocessing phase to uniform\n",
        "* Check the number of images and labels\n",
        "* Visualize some numbers"
      ],
      "metadata": {
        "id": "zzuSwdtsO9wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of training samples\n",
        "train_data.shape, train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWBVWGDSPy74",
        "outputId": "04d0b815-adc3-4c33-dd03-a8a27095fbb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of test samples\n",
        "test_data.shape, test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckmKtpRAQGPu",
        "outputId": "96a8d30b-76da-4e38-b536-9afff23676e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have,\n",
        "* 60000 samples in training data\n",
        "* 10000 samples in test data\n",
        "* input is 28 x 28 grayscale image."
      ],
      "metadata": {
        "id": "3Zx_bH8rL9mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observing the data\n",
        "# train_data[0]\n",
        "# commented this to avoid scorlling"
      ],
      "metadata": {
        "id": "CmSIQNzwRcR5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's in the form of numbers so hardly we can obtain anything by just viewing the numbers in this form, yes we can see that some indexes have 0 as value and in some it's a whole number.\n",
        "\n",
        "But we can plot these and see what these numbers tell."
      ],
      "metadata": {
        "id": "ywhJrDiAHbSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the data\n",
        "\n",
        "We can plot it using matplotlib since the values that shown above are in the form of array of numbers which has shape (1, 28, 28) of single data."
      ],
      "metadata": {
        "id": "R4bM7yKER6eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image(data, label, index):\n",
        "  image = data[index].squeeze()\n",
        "  plt.title(f\"Index {index} Label: {label[index]}\")\n",
        "  plt.imshow(image)"
      ],
      "metadata": {
        "id": "Pi21HW7qSjuI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the training image and it's label\n",
        "display_image(train_data, train_labels, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "C-WJsfXGS8eT",
        "outputId": "5832d653-c539-4ba3-dd41-742d5823796f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASLElEQVR4nO3df7DVdZ3H8ecL5JdILmQyiGhWqLlmVDd1yy0brczJKCtXbBtqLbLJWndyy3W3ZMt2nFZqqGlbMV0w80cuubKbmyE1Om1FXhxUjBRTCAhBRAQz4XJ57x/ne5sD3u/nXM5v+LweM3fuud/393u+73Pgdb6/z1cRgZkd+IZ1ugEzaw+H3SwTDrtZJhx2s0w47GaZcNjNMuGwdzlJsyXd2Ok+6tFI7/vz6+5WDnsbSFot6cxO9zEYSW+VFJKuTIwzP1XvBpI+JukxSc9J+pGkIzrdU7dx2DMmaQQwF1ja6V4aIel04F+A6cAE4Ang5k721I0c9jaT9BFJP5N0taRnJD0h6V1V9WMk3SNpu6TFwGF7TX+qpJ9L2irpgeI/OpLeJGmzpCnF368tnv/4RDufBX4M/KaB1zNX0lpJ2yQtk/SXe40yWtKtxeu5X9Jrq6Y9QtJCSU8V78Nn6mzj3cBtEfFwROwEvgy8RdIr63y+A5LD3hmnAI9QCfJXgeskqajdBCwral8GZg5MJGky8EPgSipLsEuBhZJeFhE/B64BFkgaA9wIfCEiBg2ypKOBvwG+1OBruQ+YVvRzE3CbpNFV9enAbVX1/5I0QtIw4L+BB4DJwBnAJZLeWdLvg5IuSPShQR6fWMfrOWA57J2xJiKujYh+YAEwCZgo6SjgjVRCuiMi7qUSiAF/DdwZEXdGxO6IWAz0AmcX9dnAocCvgPXAtxI9fKOYz3ONvJCIuDEino6IXRExBxgFHFc1yrKI+M+I6AO+BowGTi1e58si4ksRsTMiHgeuBc4vmc9JEXFTSRs/As6TdFLxQfdFIICDG3ltBxqHvTOeHHgQEc8XDw8BjgCeiYg/VI27purx0cAHi1X4rZK2AqdR+bCgCNR8Kku0OVFylZOkc4BxEXFroy9E0qWSVkp6tujnUPbc9Fg78CAidgPritd5NHDEXq/lcmDivvYQEXcDVwALgdXFz/ZiXlY4qNMN2B42AOMlja0K/FFUllJQCc53I+Ljg01crOZfAfwHMEfSGyNixyCjngH0SBr40DkU6Jf0moiYPtRmi+3zzxXP93BE7Jb0DHuuUk+pGn8YcCTwe2AX8ERETB3q/FIi4lsUazKSjgX+CVjRjOc+UHjJ3kUiYg2V1fJ/ljRS0mnAOVWj3AicI+mdkoZLGi3pdElHFtv884HrgAupfHB8uWRWXwCOpbKtPQ1YRGUV+qOJ9gbmN/AzEhhHJbRPAQdJ+iLwkr2me4OkcyUdBFwC7AB+SWVTY7ukz0saU7yeEyW9cSjvVbWinxNVcRQwD5gbEc/s63MdyBz27nMBlR14W6gspW8YKETEWio7vC6nErC1wN9T+Xf8DHA4le3woBLcjw6yd5yI2B4RTw78AH8E/hARWxJ9XVaMN/DzE+AuKtvLj1LZ3HiBqtX2wh3AXwHPAB8Gzo2IvmJ/xbupfNg8AWwGvkNlLeNFJD0s6UMlvY2msvPvOSofIr+g8oFmVeQvrzDLg5fsZplw2M0y4bCbZcJhN8tEW4+zj9SoGM3Yds7SLCsv8Ad2xg4NVmso7JLOonLV1HDgOxFxVWr80YzlFJ3RyCzNLGFpLCmt1b0aL2k4lTOW3gWcAMyQdEK9z2dmrdXINvvJwGMR8XhxWeEtVE74MLMu1EjYJ7Pn2VLrimF7kDRLUq+k3j4GO03bzNqh5XvjI2JeRPRERM8IRrV6dmZWopGwr6fqiiYqVzOtb6wdM2uVRsJ+HzC1+BqlkVS+dGBRc9oys2ar+9BbROySdDGVK5+GA9dHxMNN68zMmqqh4+wRcSdwZ5N6MbMW8umyZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMN3bJZ0mpgO9AP7IqInmY0ZWbN11DYC2+LiM1NeB4zayGvxptlotGwB/BjScskzRpsBEmzJPVK6u1jR4OzM7N6Nboaf1pErJd0OLBY0m8i4t7qESJiHjAP4CWaEA3Oz8zq1NCSPSLWF783AbcDJzejKTNrvrrDLmmspHEDj4F3ACua1ZiZNVcjq/ETgdslDTzPTRHxo6Z0ZWZNV3fYI+Jx4LVN7MXMWsiH3swy4bCbZcJhN8uEw26WCYfdLBPNuBDGutjOd6YvRFzzod3J+idff0+yfsn4R/e5pwGv+c6nk/WDN6RPuNz6pvTp10d/r3xZNvKu3uS0ByIv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTPg4+wHgqYv+orT2zc99Kzltz6j+ZH1YjeXBzNVnJuuvO/R3pbUHPjY3OW0ttXp704QZpbUJdzU06/2Sl+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSZ8nL0LaMTIZP2FM9Nf4rvwH/61tHbEQaOS01645u3J+pqrj0vWx/5webL+04OPKq3dc/uxyWkXTl2UrNeybflLS2sTGnrm/ZOX7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJnycvQtsuDj93e6/urTWdd/lx9I/+Ng5ySl3vb8vWT9489JkPf3N7vD7WW8orS2d2tj17P/7/Lhk/VXXrC2t7Wpozvunmkt2SddL2iRpRdWwCZIWS1pV/B7f2jbNrFFDWY2fD5y117DLgCURMRVYUvxtZl2sZtgj4l5gy16DpwMLiscLgPc2uS8za7J6t9knRsSG4vGTwMSyESXNAmYBjObgOmdnZo1qeG98RASJ/TQRMS8ieiKiZ0RiR5KZtVa9Yd8oaRJA8XtT81oys1aoN+yLgJnF45nAHc1px8xapeY2u6SbgdOBwyStA64ArgK+L+lCYA1wXiub3N+t+uYpyfoj534zWU/fQR1evfii0trxl65OTtu/+ekaz96Yiz7ZuuXAlV+ZmayPX/uLls17f1Qz7BFR9k37ZzS5FzNrIZ8ua5YJh90sEw67WSYcdrNMOOxmmfAlrk3w2zmnJuuPnJu+bfKzu19I1j/4mwuS9eM+/WhprX/79uS0tQwbOzZZf/oDJyXr0w8p/5rrYYxJTnv8bZ9K1l8134fW9oWX7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJnycfYiGTzy8tLbgff+WnHZ3jYtUax1HH/n2NTWev37Dpp2QrJ94/cpk/cqJ36gxh/JvJ3rz8vOTUx43Oz3v/hpztj15yW6WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcLH2YdIo8uPF/eMauyI75jPjEzP++gpyfqqi44srb3jzPuT0/7d4fOS9aMOSl9zXusYf3+U39RZtx6WnnbrqhrPbvvCS3azTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBM+zj5E8cKO0trSHSOS054yqi9Zv+PuW5L1WtfDN+LuP6aPda/qKz9ODvC2Mc8l6707y88h+LMb/L3v7VRzyS7pekmbJK2oGjZb0npJy4ufs1vbppk1aiir8fOBswYZ/vWImFb83Nnctsys2WqGPSLuBba0oRcza6FGdtBdLOnBYjV/fNlIkmZJ6pXU20f5dq+ZtVa9Yf828EpgGrABmFM2YkTMi4ieiOgZkfjyQTNrrbrCHhEbI6I/InYD1wInN7ctM2u2usIuaVLVn+8DVpSNa2bdoeZxdkk3A6cDh0laB1wBnC5pGhDAauATLeyxK/Rv3FRau+KTH0tOe/W/p79X/qT05ezcuC19PfuV97yntHbs/PS93w/a+GyyfvjN6X2zb5vyk2R95k/L35tj6U1Oa81VM+wRMWOQwde1oBczayGfLmuWCYfdLBMOu1kmHHazTDjsZpnwJa5NMPKu9CGky49p7TlHx/KruqfdPj3d2w+PuiNZ74v08mLM6hrHFa1tvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh4+yZ2zUm/XnfF+nbUdf6mutj5v+ufN7JKa3ZvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh4+yZG3fLL9MjlN7rx/Y3XrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpkYyi2bpwA3ABOp3KJ5XkTMlTQBuBV4OZXbNp8XEc+0rlVrhe3nn1pjjGVt6cNabyhL9l3AZyPiBOBU4FOSTgAuA5ZExFRgSfG3mXWpmmGPiA0RcX/xeDuwEpgMTAcWFKMtAN7bqibNrHH7tM0u6eXA64ClwMSI2FCUnqSymm9mXWrIYZd0CLAQuCQitlXXIiKobM8PNt0sSb2SevvY0VCzZla/IYVd0ggqQf9eRPygGLxR0qSiPgnYNNi0ETEvInoiomcEo5rRs5nVoWbYJQm4DlgZEV+rKi0CZhaPZwLp232aWUcN5RLXNwMfBh6StLwYdjlwFfB9SRcCa4DzWtOitdKzr/CpFrmoGfaI+BmgkvIZzW3HzFrFH+tmmXDYzTLhsJtlwmE3y4TDbpYJh90sE/4q6cxNvuf5ZH3ExcOT9b5BT5K2buQlu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCR9nz5z+b3myPn/b4cn6jHHrk/Xn/3xSaW3k2nXJaa25vGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh4+yW9PVrPpCsz7h0brI+6QuPldae3npSeua/fDBdt33iJbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulglFpL/4W9IU4AZgIhDAvIiYK2k28HHgqWLUyyPiztRzvUQT4hT5Ls/7k+GHvTRZH7kwfarGra/6n9LaWx+YkZx2wgVPJev9W59N1nO0NJawLbYMeov1oZxUswv4bETcL2kcsEzS4qL29Yi4ulmNmlnr1Ax7RGwANhSPt0taCUxudWNm1lz7tM0u6eXA64ClxaCLJT0o6XpJ40ummSWpV1JvHzsaatbM6jfksEs6BFgIXBIR24BvA68EplFZ8s8ZbLqImBcRPRHRM4JRTWjZzOoxpLBLGkEl6N+LiB8ARMTGiOiPiN3AtcDJrWvTzBpVM+ySBFwHrIyIr1UNr/7a0PcBK5rfnpk1y1D2xr8Z+DDwkKSB7x2+HJghaRqVw3GrgU+0pEPrqP7NTyfrO9+fPjT36jnl/y1WnnlNctr3HH9hsu5LYPfNUPbG/wwY7Lhd8pi6mXUXn0FnlgmH3SwTDrtZJhx2s0w47GaZcNjNMlHzEtdm8iWuZq2VusTVS3azTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBNtPc4u6SlgTdWgw4DNbWtg33Rrb93aF7i3ejWzt6Mj4mWDFdoa9hfNXOqNiJ6ONZDQrb11a1/g3urVrt68Gm+WCYfdLBOdDvu8Ds8/pVt769a+wL3Vqy29dXSb3czap9NLdjNrE4fdLBMdCbuksyQ9IukxSZd1oocyklZLekjSckm9He7lekmbJK2oGjZB0mJJq4rfg95jr0O9zZa0vnjvlks6u0O9TZH0U0m/lvSwpL8thnf0vUv01Zb3re3b7JKGA48CbwfWAfcBMyLi121tpISk1UBPRHT8BAxJbwGeA26IiBOLYV8FtkTEVcUH5fiI+HyX9DYbeK7Tt/Eu7lY0qfo248B7gY/Qwfcu0dd5tOF968SS/WTgsYh4PCJ2ArcA0zvQR9eLiHuBLXsNng4sKB4voPKfpe1KeusKEbEhIu4vHm8HBm4z3tH3LtFXW3Qi7JOBtVV/r6O77vcewI8lLZM0q9PNDGJiRGwoHj8JTOxkM4OoeRvvdtrrNuNd897Vc/vzRnkH3YudFhGvB94FfKpYXe1KUdkG66Zjp0O6jXe7DHKb8T/p5HtX7+3PG9WJsK8HplT9fWQxrCtExPri9ybgdrrvVtQbB+6gW/ze1OF+/qSbbuM92G3G6YL3rpO3P+9E2O8Dpko6RtJI4HxgUQf6eBFJY4sdJ0gaC7yD7rsV9SJgZvF4JnBHB3vZQ7fcxrvsNuN0+L3r+O3PI6LtP8DZVPbI/xb4x070UNLXK4AHip+HO90bcDOV1bo+Kvs2LgReCiwBVgF3AxO6qLfvAg8BD1IJ1qQO9XYalVX0B4Hlxc/ZnX7vEn215X3z6bJmmfAOOrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sE/8PBlmO9e/wT8sAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_image(train_data, train_labels, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "JFK_osqLYf6g",
        "outputId": "7f754494-3b39-4fcc-c328-d8c2e40917af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's identify how many labels we have in our training set and their corresponding images."
      ],
      "metadata": {
        "id": "oLuRjAvcY_9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_counts = np.unique(train_labels, return_counts=True)\n",
        "train_label_df = pd.DataFrame({\"Label\": train_label_counts[0], \"Count\":train_label_counts[1]})\n",
        "train_label_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "dmQI4TJsZVdb",
        "outputId": "ffba00b0-5e84-4095-a8f3-c6ca51289a2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Label  Count\n",
              "0      0   5923\n",
              "1      1   6742\n",
              "2      2   5958\n",
              "3      3   6131\n",
              "4      4   5842\n",
              "5      5   5421\n",
              "6      6   5918\n",
              "7      7   6265\n",
              "8      8   5851\n",
              "9      9   5949"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34746573-cdc5-4d75-a4e4-0f3ab9208c69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>5918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>6265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>5851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>5949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34746573-cdc5-4d75-a4e4-0f3ab9208c69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34746573-cdc5-4d75-a4e4-0f3ab9208c69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34746573-cdc5-4d75-a4e4-0f3ab9208c69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It says, Digit `1` has 6742 samples, `2` has 5958, `3` has 6131 samples and so on... available in training dataset.\n",
        "\n",
        "So these are basically our classnames for multi-class classification problem."
      ],
      "metadata": {
        "id": "FVPzOuuQZcLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_label_df[\"Label\"]"
      ],
      "metadata": {
        "id": "yzAYjSVJJ2TQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpxSbo-EKMps",
        "outputId": "5b9dd7fa-ae53-465a-ae5b-cdc51bcd9bd1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also by observing the data it looks like that it's already converted into numbers and it's ready to fit in into the model.\n",
        "\n",
        "\n",
        "### Split the data into 3 set (Training, Validation and Test)\n",
        "\n",
        "It's a good practice in order to build our model, we should split the samples into 3 sets, (Training, Validation and Test set).\n",
        "\n",
        "Since we already have the test set available, so we only need to divide the validation test which we split it from training samples.\n",
        "\n",
        "* Training Samples : (48K)\n",
        "* Validation Samples : (12K)\n",
        "* Test Samples : (10K)"
      ],
      "metadata": {
        "id": "AMXkrLoxanRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing train test split library from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.2, random_state=17)"
      ],
      "metadata": {
        "id": "p1pynydlcKx_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the number of splitted data\n",
        "X_train.shape[0], X_valid.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yhIfl4CcPYW",
        "outputId": "73280b59-298f-41a6-d12b-d63ea1161933"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 12000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and now if we compare the lables in each set, after splitting"
      ],
      "metadata": {
        "id": "PCk4x0sXcrSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train, return_counts=True), np.unique(y_valid, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o8o-Cv-c-oc",
        "outputId": "d0d972af-2f4a-4d1e-dee7-f220b40afaa6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              "  array([4719, 5380, 4754, 4956, 4686, 4301, 4734, 5047, 4620, 4803])),\n",
              " (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              "  array([1204, 1362, 1204, 1175, 1156, 1120, 1184, 1218, 1231, 1146])))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have good amount of data to train our model and then validate it using validation set."
      ],
      "metadata": {
        "id": "O2pPfLlXLkf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling\n",
        "\n",
        "We're dealing with Multi-class classification problem, so we might need to take care of few things to build our neural network model,\n",
        "\n",
        "* **Creating a model**: piece together the layers of neural network(using the functional or Sequential API) or import a previously built model(known as transfer learning).\n",
        "  * **Input shape**: We will have to deal with 28 x 28 tensors (height and width of an image)\n",
        "  * **Output shape**: 10, since we have 10 `class_names`(*digits*) to identify\n",
        "  * **Input Activation function**: Since this is non-linear data, se we'll use non-linear activations like reLu, Softmax, Sigmoid\n",
        "  * **Output Activation function**: For classification problems, two common functions are Sigmoid and Softmax, since this is a multi-class so we should use *Softmax*\n",
        "    - **Softmax** : It converts all the output vector into probabilities that the sum of all the probabilities is equal to 1. Kind of argmax, but softer version.\n",
        "* **Compiling a model**: defining how a model's performance should be measured(loss/metrics) as well as defining how it should improve(optimizer).\n",
        "  * **Loss function**: We have to use **CategoricalCrossentropy** or **SparseCategoricalCrossentropy** for multi-class classification problem.\n",
        "    - We will use the later, i.e *SparseCategoricalCrossentropy* since the other one takes encoded inputs, and we are not `one_hot` encoding our training data.\n",
        "  * **Optimizer**: We can choose from SGD or Adam optimizer functions\n",
        "  * **Metrics**: For a classification problem, there are many metrics but by default let's see `accuracy`. How accurate our model is performing.\n",
        "* **Fitting a model**: Letting the model try to find patterns in the data\n",
        "\n",
        "Let's see how it'll go."
      ],
      "metadata": {
        "id": "fADcEmmRmx5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default Model"
      ],
      "metadata": {
        "id": "7_s3uxmbv3AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# 1. Create a model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # input shape define\n",
        "    tf.keras.layers.Dense(4, activation='relu'), # Hidden layer 1 with 4 neurons\n",
        "    tf.keras.layers.Dense(4, activation='relu'), # Hidden layer 2 with 4 neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # Output layer, since shape is 10 so 10 neurons.\n",
        "])\n",
        "\n",
        "# 2. Compiling a model\n",
        "model_1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# 3. Fitting a model\n",
        "history_1 = model_1.fit(X_train, \n",
        "            y_train, \n",
        "            epochs = 20, \n",
        "            validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7OojIOrO533",
        "outputId": "06ab79e3-a3d3-4cca-e2e7-9d2967d94b7b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 2.3384 - accuracy: 0.1562 - val_loss: 1.9453 - val_accuracy: 0.2059\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.8860 - accuracy: 0.2173 - val_loss: 1.8386 - val_accuracy: 0.2477\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7600 - accuracy: 0.2725 - val_loss: 1.6868 - val_accuracy: 0.2825\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.6135 - accuracy: 0.3251 - val_loss: 1.5253 - val_accuracy: 0.3722\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.4784 - accuracy: 0.4133 - val_loss: 1.4424 - val_accuracy: 0.4397\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.4167 - accuracy: 0.4407 - val_loss: 1.3916 - val_accuracy: 0.4448\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3721 - accuracy: 0.4601 - val_loss: 1.3720 - val_accuracy: 0.4658\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3425 - accuracy: 0.4754 - val_loss: 1.3312 - val_accuracy: 0.4863\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2969 - accuracy: 0.4995 - val_loss: 1.2765 - val_accuracy: 0.5252\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2533 - accuracy: 0.5306 - val_loss: 1.2365 - val_accuracy: 0.5460\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2093 - accuracy: 0.5590 - val_loss: 1.2066 - val_accuracy: 0.5779\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1694 - accuracy: 0.5823 - val_loss: 1.1780 - val_accuracy: 0.5823\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1281 - accuracy: 0.6071 - val_loss: 1.1206 - val_accuracy: 0.6267\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0986 - accuracy: 0.6305 - val_loss: 1.1200 - val_accuracy: 0.6388\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0876 - accuracy: 0.6413 - val_loss: 1.0836 - val_accuracy: 0.6472\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0664 - accuracy: 0.6527 - val_loss: 1.0932 - val_accuracy: 0.6527\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0562 - accuracy: 0.6570 - val_loss: 1.0657 - val_accuracy: 0.6524\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0517 - accuracy: 0.6582 - val_loss: 1.1165 - val_accuracy: 0.6271\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0455 - accuracy: 0.6596 - val_loss: 1.0473 - val_accuracy: 0.6668\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0410 - accuracy: 0.6630 - val_loss: 1.0768 - val_accuracy: 0.6571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy on training samples is **66%** and loss is fairly low. But we still need to improve our model to get better `accuracy`.\n",
        "\n",
        "One more thing we might notice here, `val_accuracy`, these appears in the process of fitting our model, because we passed a parameter i.e `validation_data`.\n",
        "\n",
        "It gives us the idea of how the model performs on validation set during training the model, it gives us the `val_loss` and `val_accuracy`"
      ],
      "metadata": {
        "id": "1gRc_SLzdtqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anyways let's improve our model."
      ],
      "metadata": {
        "id": "2IvThJjVd-1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning our Model\n",
        "\n",
        "A model can tuned/improved in many ways but some common ways we can pass through will be,\n",
        "* Visualize our inputs, *if normalization needed then normalize it*\n",
        "* *Adjust the layers and activation function*\n",
        "* *Tweak optimizer and it's learning rate*\n",
        "* and Many more.."
      ],
      "metadata": {
        "id": "6Be6_N-zfRht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize our inputs\n",
        "\n",
        "> Why do we need to normalize our inputs ?\n",
        "\n",
        "So if we plot the distribution of our inputs, it ranges from 0~255 which is ideally not good for machines to learn the patterns from this spread of distribution."
      ],
      "metadata": {
        "id": "UUSv-dEifvSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.min(), X_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCB78dAmPew0",
        "outputId": "5b59dd86-f634-4326-8260-859340b1c328"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.min(), X_valid.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA_9QGtbPjoI",
        "outputId": "8274dc90-4cff-4d84-cc57-29360bd7224d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So when we apply normalization, we shrink that spread between 0~1. In our case, if we want to do this we can simply divide all the data with our maximum number i.e 255.\n"
      ],
      "metadata": {
        "id": "btxnoQM8Prd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's normalize our training and valid data\n",
        "X_train_norm = X_train / X_train.max()\n",
        "X_valid_norm = X_valid / X_valid.max()"
      ],
      "metadata": {
        "id": "o-3Y0O_XP8D5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Also let us normalize test data\n",
        "test_data_norm = test_data / test_data.max()"
      ],
      "metadata": {
        "id": "iSCvhuQTQJ-g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the normalize data\n",
        "X_train_norm[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ9DaWFQQWr5",
        "outputId": "5eb77d87-d976-4cf6-d414-a7933d55684e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.03137255, 0.30588235,\n",
              "        0.58039216, 0.68235294, 0.81960784, 0.75686275, 0.14509804,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.44705882, 0.98823529,\n",
              "        0.98823529, 0.98823529, 0.98823529, 0.99215686, 0.66666667,\n",
              "        0.02745098, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.38431373, 0.90980392, 0.80392157,\n",
              "        0.69803922, 0.21960784, 0.25490196, 0.99215686, 0.98823529,\n",
              "        0.4627451 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.41568627, 0.54901961, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.61176471, 0.98823529,\n",
              "        0.76862745, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.07058824, 0.07058824, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.09803922, 0.88627451,\n",
              "        0.97647059, 0.2745098 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.74901961,\n",
              "        0.99215686, 0.32941176, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.74509804,\n",
              "        0.98823529, 0.32941176, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.74509804,\n",
              "        0.98823529, 0.32941176, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.74509804,\n",
              "        0.98823529, 0.32941176, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.14117647, 0.94901961,\n",
              "        0.95686275, 0.19215686, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.82352941, 0.99215686,\n",
              "        0.66666667, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.14117647, 0.99215686, 0.98823529,\n",
              "        0.35686275, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.03529412, 0.08627451, 0.08627451,\n",
              "        0.08627451, 0.0627451 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.16078431, 0.8745098 , 0.99215686, 0.78823529,\n",
              "        0.04313725, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.35686275, 0.8       , 0.99215686, 0.98823529,\n",
              "        0.98823529, 0.90588235, 0.49411765, 0.        , 0.        ,\n",
              "        0.16078431, 0.88235294, 0.98823529, 0.95294118, 0.18431373,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.14117647,\n",
              "        0.80784314, 0.98823529, 0.98823529, 0.74901961, 0.57647059,\n",
              "        0.95686275, 0.98823529, 0.98823529, 0.76078431, 0.41568627,\n",
              "        0.8745098 , 0.98823529, 0.98823529, 0.1372549 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.74901961,\n",
              "        0.99215686, 0.84705882, 0.24313725, 0.        , 0.        ,\n",
              "        0.03529412, 0.55686275, 0.99215686, 1.        , 0.99215686,\n",
              "        0.99215686, 0.78039216, 0.07058824, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.74509804,\n",
              "        0.96078431, 0.32156863, 0.        , 0.        , 0.        ,\n",
              "        0.16862745, 0.49803922, 0.98823529, 0.99215686, 0.98823529,\n",
              "        0.95686275, 0.19215686, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.74509804,\n",
              "        0.94901961, 0.49803922, 0.49803922, 0.50196078, 0.63529412,\n",
              "        0.94901961, 0.98823529, 0.98823529, 0.9254902 , 0.98823529,\n",
              "        0.98823529, 0.83137255, 0.49803922, 0.74117647, 0.6       ,\n",
              "        0.04313725, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.63921569,\n",
              "        0.98823529, 0.98823529, 0.98823529, 0.99215686, 0.98823529,\n",
              "        0.98823529, 0.71764706, 0.32941176, 0.05490196, 0.63921569,\n",
              "        0.96862745, 0.98823529, 0.98823529, 0.95294118, 0.6       ,\n",
              "        0.02745098, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "        0.57647059, 0.98823529, 0.81568627, 0.57647059, 0.43921569,\n",
              "        0.16470588, 0.05490196, 0.        , 0.        , 0.        ,\n",
              "        0.14901961, 0.44313725, 0.57647059, 0.1372549 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and if we see the normalize max and min\n",
        "tf.reduce_min(X_train_norm), tf.reduce_max(X_train_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suqo5W4aQa9-",
        "outputId": "291ef75c-5999-41f9-cc6c-8a962c41e278"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float64, numpy=0.0>,\n",
              " <tf.Tensor: shape=(), dtype=float64, numpy=1.0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build our model again with normalized data"
      ],
      "metadata": {
        "id": "4aiflRzKQlp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Create a model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile a model\n",
        "model_2.compile(loss='sparse_categorical_crossentropy',  # we can write in this form as well\n",
        "                optimizer='Adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit our model\n",
        "history_2 = model_2.fit(X_train_norm,\n",
        "                        y_train,\n",
        "                        epochs = 20,\n",
        "                        validation_data=(X_valid_norm, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVCKlOMpQtWC",
        "outputId": "92ddd6e4-0522-4fa2-b449-bcace8179303"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 3s 1ms/step - loss: 1.4868 - accuracy: 0.4670 - val_loss: 0.9892 - val_accuracy: 0.7036\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8228 - accuracy: 0.7546 - val_loss: 0.7323 - val_accuracy: 0.7837\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7065 - accuracy: 0.7875 - val_loss: 0.6910 - val_accuracy: 0.7921\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6661 - accuracy: 0.8008 - val_loss: 0.6584 - val_accuracy: 0.8046\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6395 - accuracy: 0.8078 - val_loss: 0.6314 - val_accuracy: 0.8148\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6170 - accuracy: 0.8138 - val_loss: 0.6209 - val_accuracy: 0.8179\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5966 - accuracy: 0.8207 - val_loss: 0.5997 - val_accuracy: 0.8263\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5748 - accuracy: 0.8284 - val_loss: 0.5784 - val_accuracy: 0.8324\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5508 - accuracy: 0.8375 - val_loss: 0.5514 - val_accuracy: 0.8428\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5328 - accuracy: 0.8447 - val_loss: 0.5572 - val_accuracy: 0.8393\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5171 - accuracy: 0.8501 - val_loss: 0.5492 - val_accuracy: 0.8397\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5050 - accuracy: 0.8527 - val_loss: 0.5250 - val_accuracy: 0.8535\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4960 - accuracy: 0.8553 - val_loss: 0.5109 - val_accuracy: 0.8581\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4881 - accuracy: 0.8581 - val_loss: 0.5120 - val_accuracy: 0.8520\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4796 - accuracy: 0.8612 - val_loss: 0.5043 - val_accuracy: 0.8590\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4741 - accuracy: 0.8629 - val_loss: 0.4922 - val_accuracy: 0.8635\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4680 - accuracy: 0.8660 - val_loss: 0.4884 - val_accuracy: 0.8635\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4628 - accuracy: 0.8668 - val_loss: 0.4912 - val_accuracy: 0.8633\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4587 - accuracy: 0.8689 - val_loss: 0.4883 - val_accuracy: 0.8601\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4544 - accuracy: 0.8694 - val_loss: 0.4919 - val_accuracy: 0.8602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have increased our Accuracy of `model_1` of **66%** to **86%** of `model_2`, just by normalizing out data. That's not Bad.\n",
        "\n",
        "What else we can do to tune our model ?\n",
        "\n",
        "We can tweak our optimization function.\n",
        "\n",
        "We have options like SGD(), Adam and many more optimizer functions, but according to the studies it's found that Adam performs fairly good and cover almost every kind of problem. \n",
        "\n",
        "So why trying every other optimization functions, if we can try tweaking some parameters with their different values of one function i.e **Adam()**.\n",
        "\n",
        "There's a very crucial parameter called, `learning_rate`, in Adam. It means, how fast the model learn and optimizer function helps it to optimize the learning capabilities.\n",
        "\n",
        "How about we find the ideal learning rate and see what happens ?"
      ],
      "metadata": {
        "id": "x4GVwYc8Re3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Ideal learning rate ?\n",
        "\n",
        "We'll use the same architecture we've been using.\n",
        "\n",
        "We need to create a callback i.e [LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) which takes the current learning rate as the input and returns the next learning rate at the beginning of every epochs."
      ],
      "metadata": {
        "id": "vBtSQ4gkSgDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Create a model\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile a model\n",
        "model_3.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Create a learning rate callback\n",
        "lr_schedular = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
        "\n",
        "# Fit the model\n",
        "history_3 = model_3.fit(X_train_norm,\n",
        "                        y_train,\n",
        "                        epochs=80,\n",
        "                        validation_data=(X_valid_norm, y_valid),\n",
        "                        callbacks=[lr_schedular])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOC2T5PaTnXw",
        "outputId": "2948901d-a2d7-473a-a62b-e6d310bcddf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 2.1666 - accuracy: 0.2242 - val_loss: 2.0271 - val_accuracy: 0.3038 - lr: 1.0000e-04\n",
            "Epoch 2/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.9224 - accuracy: 0.3338 - val_loss: 1.8221 - val_accuracy: 0.3707 - lr: 1.1220e-04\n",
            "Epoch 3/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7257 - accuracy: 0.3834 - val_loss: 1.6241 - val_accuracy: 0.3954 - lr: 1.2589e-04\n",
            "Epoch 4/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5457 - accuracy: 0.4195 - val_loss: 1.4718 - val_accuracy: 0.4412 - lr: 1.4125e-04\n",
            "Epoch 5/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.4052 - accuracy: 0.4860 - val_loss: 1.3378 - val_accuracy: 0.5257 - lr: 1.5849e-04\n",
            "Epoch 6/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 1.2692 - accuracy: 0.5670 - val_loss: 1.2051 - val_accuracy: 0.5957 - lr: 1.7783e-04\n",
            "Epoch 7/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1522 - accuracy: 0.6331 - val_loss: 1.1031 - val_accuracy: 0.6571 - lr: 1.9953e-04\n",
            "Epoch 8/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0710 - accuracy: 0.6733 - val_loss: 1.0368 - val_accuracy: 0.6888 - lr: 2.2387e-04\n",
            "Epoch 9/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0154 - accuracy: 0.6960 - val_loss: 0.9912 - val_accuracy: 0.7078 - lr: 2.5119e-04\n",
            "Epoch 10/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9737 - accuracy: 0.7128 - val_loss: 0.9586 - val_accuracy: 0.7181 - lr: 2.8184e-04\n",
            "Epoch 11/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9411 - accuracy: 0.7238 - val_loss: 0.9304 - val_accuracy: 0.7265 - lr: 3.1623e-04\n",
            "Epoch 12/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9128 - accuracy: 0.7326 - val_loss: 0.9054 - val_accuracy: 0.7366 - lr: 3.5481e-04\n",
            "Epoch 13/80\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8873 - accuracy: 0.7380 - val_loss: 0.8772 - val_accuracy: 0.7435 - lr: 3.9811e-04\n",
            "Epoch 14/80\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8637 - accuracy: 0.7455 - val_loss: 0.8587 - val_accuracy: 0.7463 - lr: 4.4668e-04\n",
            "Epoch 15/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.8408 - accuracy: 0.7512 - val_loss: 0.8369 - val_accuracy: 0.7536 - lr: 5.0119e-04\n",
            "Epoch 16/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8184 - accuracy: 0.7569 - val_loss: 0.8144 - val_accuracy: 0.7624 - lr: 5.6234e-04\n",
            "Epoch 17/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7963 - accuracy: 0.7641 - val_loss: 0.7942 - val_accuracy: 0.7634 - lr: 6.3096e-04\n",
            "Epoch 18/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7758 - accuracy: 0.7692 - val_loss: 0.7763 - val_accuracy: 0.7682 - lr: 7.0795e-04\n",
            "Epoch 19/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7551 - accuracy: 0.7750 - val_loss: 0.7626 - val_accuracy: 0.7723 - lr: 7.9433e-04\n",
            "Epoch 20/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7354 - accuracy: 0.7808 - val_loss: 0.7391 - val_accuracy: 0.7809 - lr: 8.9125e-04\n",
            "Epoch 21/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7151 - accuracy: 0.7899 - val_loss: 0.7217 - val_accuracy: 0.7886 - lr: 0.0010\n",
            "Epoch 22/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6954 - accuracy: 0.7983 - val_loss: 0.7002 - val_accuracy: 0.7968 - lr: 0.0011\n",
            "Epoch 23/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6713 - accuracy: 0.8051 - val_loss: 0.6710 - val_accuracy: 0.8044 - lr: 0.0013\n",
            "Epoch 24/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6513 - accuracy: 0.8096 - val_loss: 0.6622 - val_accuracy: 0.8060 - lr: 0.0014\n",
            "Epoch 25/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6366 - accuracy: 0.8140 - val_loss: 0.6743 - val_accuracy: 0.7998 - lr: 0.0016\n",
            "Epoch 26/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6295 - accuracy: 0.8155 - val_loss: 0.6477 - val_accuracy: 0.8153 - lr: 0.0018\n",
            "Epoch 27/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6231 - accuracy: 0.8180 - val_loss: 0.6460 - val_accuracy: 0.8096 - lr: 0.0020\n",
            "Epoch 28/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6179 - accuracy: 0.8203 - val_loss: 0.6390 - val_accuracy: 0.8152 - lr: 0.0022\n",
            "Epoch 29/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6142 - accuracy: 0.8190 - val_loss: 0.6164 - val_accuracy: 0.8210 - lr: 0.0025\n",
            "Epoch 30/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6093 - accuracy: 0.8219 - val_loss: 0.6261 - val_accuracy: 0.8212 - lr: 0.0028\n",
            "Epoch 31/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6071 - accuracy: 0.8217 - val_loss: 0.6274 - val_accuracy: 0.8200 - lr: 0.0032\n",
            "Epoch 32/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6059 - accuracy: 0.8219 - val_loss: 0.6171 - val_accuracy: 0.8223 - lr: 0.0035\n",
            "Epoch 33/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6006 - accuracy: 0.8247 - val_loss: 0.6136 - val_accuracy: 0.8204 - lr: 0.0040\n",
            "Epoch 34/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5930 - accuracy: 0.8263 - val_loss: 0.5969 - val_accuracy: 0.8321 - lr: 0.0045\n",
            "Epoch 35/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5851 - accuracy: 0.8290 - val_loss: 0.5935 - val_accuracy: 0.8286 - lr: 0.0050\n",
            "Epoch 36/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5801 - accuracy: 0.8310 - val_loss: 0.5766 - val_accuracy: 0.8367 - lr: 0.0056\n",
            "Epoch 37/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5752 - accuracy: 0.8321 - val_loss: 0.5794 - val_accuracy: 0.8407 - lr: 0.0063\n",
            "Epoch 38/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5656 - accuracy: 0.8364 - val_loss: 0.5663 - val_accuracy: 0.8394 - lr: 0.0071\n",
            "Epoch 39/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5577 - accuracy: 0.8384 - val_loss: 0.5468 - val_accuracy: 0.8444 - lr: 0.0079\n",
            "Epoch 40/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5478 - accuracy: 0.8409 - val_loss: 0.5552 - val_accuracy: 0.8419 - lr: 0.0089\n",
            "Epoch 41/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.8458 - val_loss: 0.5322 - val_accuracy: 0.8549 - lr: 0.0100\n",
            "Epoch 42/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5279 - accuracy: 0.8459 - val_loss: 0.5426 - val_accuracy: 0.8459 - lr: 0.0112\n",
            "Epoch 43/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5332 - accuracy: 0.8428 - val_loss: 0.5709 - val_accuracy: 0.8350 - lr: 0.0126\n",
            "Epoch 44/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5386 - accuracy: 0.8416 - val_loss: 0.5346 - val_accuracy: 0.8438 - lr: 0.0141\n",
            "Epoch 45/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5454 - accuracy: 0.8405 - val_loss: 0.5380 - val_accuracy: 0.8398 - lr: 0.0158\n",
            "Epoch 46/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5481 - accuracy: 0.8379 - val_loss: 0.5732 - val_accuracy: 0.8376 - lr: 0.0178\n",
            "Epoch 47/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5554 - accuracy: 0.8374 - val_loss: 0.5438 - val_accuracy: 0.8425 - lr: 0.0200\n",
            "Epoch 48/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5751 - accuracy: 0.8328 - val_loss: 0.5941 - val_accuracy: 0.8207 - lr: 0.0224\n",
            "Epoch 49/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5808 - accuracy: 0.8313 - val_loss: 0.6514 - val_accuracy: 0.8013 - lr: 0.0251\n",
            "Epoch 50/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6044 - accuracy: 0.8229 - val_loss: 0.5761 - val_accuracy: 0.8421 - lr: 0.0282\n",
            "Epoch 51/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6230 - accuracy: 0.8192 - val_loss: 0.6345 - val_accuracy: 0.8208 - lr: 0.0316\n",
            "Epoch 52/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6509 - accuracy: 0.8128 - val_loss: 0.6653 - val_accuracy: 0.8088 - lr: 0.0355\n",
            "Epoch 53/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6880 - accuracy: 0.8039 - val_loss: 0.7795 - val_accuracy: 0.7871 - lr: 0.0398\n",
            "Epoch 54/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7051 - accuracy: 0.7994 - val_loss: 0.7094 - val_accuracy: 0.7968 - lr: 0.0447\n",
            "Epoch 55/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7673 - accuracy: 0.7819 - val_loss: 0.8224 - val_accuracy: 0.7513 - lr: 0.0501\n",
            "Epoch 56/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.8126 - accuracy: 0.7691 - val_loss: 0.9141 - val_accuracy: 0.7405 - lr: 0.0562\n",
            "Epoch 57/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9047 - accuracy: 0.7253 - val_loss: 0.9613 - val_accuracy: 0.6812 - lr: 0.0631\n",
            "Epoch 58/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0934 - accuracy: 0.6394 - val_loss: 1.0624 - val_accuracy: 0.6898 - lr: 0.0708\n",
            "Epoch 59/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1369 - accuracy: 0.6442 - val_loss: 1.0170 - val_accuracy: 0.7190 - lr: 0.0794\n",
            "Epoch 60/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3044 - accuracy: 0.5801 - val_loss: 1.4991 - val_accuracy: 0.4023 - lr: 0.0891\n",
            "Epoch 61/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5564 - accuracy: 0.4245 - val_loss: 1.7965 - val_accuracy: 0.2929 - lr: 0.1000\n",
            "Epoch 62/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7917 - accuracy: 0.2968 - val_loss: 2.0255 - val_accuracy: 0.1947 - lr: 0.1122\n",
            "Epoch 63/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.0656 - accuracy: 0.1897 - val_loss: 2.1402 - val_accuracy: 0.1796 - lr: 0.1259\n",
            "Epoch 64/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.1289 - accuracy: 0.1878 - val_loss: 2.1439 - val_accuracy: 0.1947 - lr: 0.1413\n",
            "Epoch 65/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.2249 - accuracy: 0.1425 - val_loss: 2.1552 - val_accuracy: 0.1725 - lr: 0.1585\n",
            "Epoch 66/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.9435 - accuracy: 0.1382 - val_loss: 2.3281 - val_accuracy: 0.1003 - lr: 0.1778\n",
            "Epoch 67/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3265 - accuracy: 0.1038 - val_loss: 2.3173 - val_accuracy: 0.1015 - lr: 0.1995\n",
            "Epoch 68/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 2.3303 - accuracy: 0.1024 - val_loss: 2.3459 - val_accuracy: 0.1015 - lr: 0.2239\n",
            "Epoch 69/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3307 - accuracy: 0.1010 - val_loss: 2.3411 - val_accuracy: 0.1015 - lr: 0.2512\n",
            "Epoch 70/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3346 - accuracy: 0.0992 - val_loss: 2.3202 - val_accuracy: 0.0979 - lr: 0.2818\n",
            "Epoch 71/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3394 - accuracy: 0.1008 - val_loss: 2.3479 - val_accuracy: 0.0979 - lr: 0.3162\n",
            "Epoch 72/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 2.3424 - accuracy: 0.1026 - val_loss: 2.3498 - val_accuracy: 0.0963 - lr: 0.3548\n",
            "Epoch 73/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3486 - accuracy: 0.1011 - val_loss: 2.3708 - val_accuracy: 0.0955 - lr: 0.3981\n",
            "Epoch 74/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3549 - accuracy: 0.1000 - val_loss: 2.3631 - val_accuracy: 0.0933 - lr: 0.4467\n",
            "Epoch 75/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3613 - accuracy: 0.1007 - val_loss: 2.3294 - val_accuracy: 0.1003 - lr: 0.5012\n",
            "Epoch 76/80\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 2.3621 - accuracy: 0.1014 - val_loss: 2.4053 - val_accuracy: 0.1003 - lr: 0.5623\n",
            "Epoch 77/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3732 - accuracy: 0.1030 - val_loss: 2.3193 - val_accuracy: 0.1135 - lr: 0.6310\n",
            "Epoch 78/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3837 - accuracy: 0.0993 - val_loss: 2.3732 - val_accuracy: 0.0963 - lr: 0.7079\n",
            "Epoch 79/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3922 - accuracy: 0.1011 - val_loss: 2.3704 - val_accuracy: 0.0955 - lr: 0.7943\n",
            "Epoch 80/80\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3988 - accuracy: 0.0993 - val_loss: 2.4216 - val_accuracy: 0.1135 - lr: 0.8913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal was to find the ideal learning rate, How we can obtain that ?\n",
        "\n",
        "Notice we got a new return value `lr` with some exponetial values, those are the different learning rate on which each epoch has run. But let's find out the ideal learning rate.......\n",
        "\n",
        "By plotting the *learning rate decay*."
      ],
      "metadata": {
        "id": "Y-X7x_-vV0e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80 # we chose 80 epochs to fit our model\n",
        "lrs = 1e-4 * (10**(tf.range(epochs)/20)) # this is the same lambda variable which we declared with LearningRateScheduler\n",
        "plt.semilogx(lrs, history_3.history['loss']) # we accessing here the loss value from history that we recieved after fitting the model\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Finding the ideal learning rate\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "iQ5Frc2RWEXR",
        "outputId": "eb12927a-2733-49ae-fabd-5f784b14f1b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's been observed that, the optimal learning rate is the range between the lowest point in the curve i.e ~0.01 and 10x behind that point i.e ~0.001\n",
        "\n",
        "And 0.001 is the default value of `learning_rate` of [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) which we have been training our model with these default values. But for completeness let's build our model again with `0.01`"
      ],
      "metadata": {
        "id": "y0dd7Nf1XO55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# create a model\n",
        "model_4 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile a model\n",
        "model_4.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# fit the model\n",
        "history_4 = model_4.fit(X_train_norm,\n",
        "                        y_train,\n",
        "                        epochs=100, # This time for longer period of epochs\n",
        "                        validation_data = (X_valid_norm, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBbTnal6ax8v",
        "outputId": "2040b4b2-b31e-4e14-f2e3-eb3c18b6215d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8602 - accuracy: 0.7186 - val_loss: 0.6236 - val_accuracy: 0.8092\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6036 - accuracy: 0.8173 - val_loss: 0.5593 - val_accuracy: 0.8309\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5505 - accuracy: 0.8358 - val_loss: 0.5598 - val_accuracy: 0.8360\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5190 - accuracy: 0.8485 - val_loss: 0.5140 - val_accuracy: 0.8491\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5032 - accuracy: 0.8520 - val_loss: 0.5004 - val_accuracy: 0.8571\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4961 - accuracy: 0.8547 - val_loss: 0.4987 - val_accuracy: 0.8532\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4897 - accuracy: 0.8587 - val_loss: 0.5170 - val_accuracy: 0.8461\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4825 - accuracy: 0.8577 - val_loss: 0.5021 - val_accuracy: 0.8540\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4762 - accuracy: 0.8601 - val_loss: 0.5005 - val_accuracy: 0.8499\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4750 - accuracy: 0.8608 - val_loss: 0.5162 - val_accuracy: 0.8506\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4726 - accuracy: 0.8623 - val_loss: 0.4835 - val_accuracy: 0.8602\n",
            "Epoch 12/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4669 - accuracy: 0.8647 - val_loss: 0.4989 - val_accuracy: 0.8554\n",
            "Epoch 13/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4663 - accuracy: 0.8631 - val_loss: 0.5139 - val_accuracy: 0.8518\n",
            "Epoch 14/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4637 - accuracy: 0.8625 - val_loss: 0.5030 - val_accuracy: 0.8561\n",
            "Epoch 15/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4602 - accuracy: 0.8642 - val_loss: 0.4833 - val_accuracy: 0.8600\n",
            "Epoch 16/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4604 - accuracy: 0.8635 - val_loss: 0.4882 - val_accuracy: 0.8600\n",
            "Epoch 17/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4563 - accuracy: 0.8652 - val_loss: 0.4956 - val_accuracy: 0.8511\n",
            "Epoch 18/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4571 - accuracy: 0.8649 - val_loss: 0.5020 - val_accuracy: 0.8540\n",
            "Epoch 19/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4564 - accuracy: 0.8644 - val_loss: 0.5169 - val_accuracy: 0.8493\n",
            "Epoch 20/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4540 - accuracy: 0.8671 - val_loss: 0.5245 - val_accuracy: 0.8404\n",
            "Epoch 21/100\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4553 - accuracy: 0.8672 - val_loss: 0.5068 - val_accuracy: 0.8544\n",
            "Epoch 22/100\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4505 - accuracy: 0.8679 - val_loss: 0.4673 - val_accuracy: 0.8629\n",
            "Epoch 23/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4526 - accuracy: 0.8667 - val_loss: 0.4944 - val_accuracy: 0.8552\n",
            "Epoch 24/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4504 - accuracy: 0.8664 - val_loss: 0.4786 - val_accuracy: 0.8605\n",
            "Epoch 25/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4513 - accuracy: 0.8665 - val_loss: 0.4770 - val_accuracy: 0.8599\n",
            "Epoch 26/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4496 - accuracy: 0.8671 - val_loss: 0.5507 - val_accuracy: 0.8397\n",
            "Epoch 27/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4505 - accuracy: 0.8661 - val_loss: 0.4942 - val_accuracy: 0.8511\n",
            "Epoch 28/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4481 - accuracy: 0.8681 - val_loss: 0.4914 - val_accuracy: 0.8593\n",
            "Epoch 29/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4470 - accuracy: 0.8686 - val_loss: 0.4718 - val_accuracy: 0.8656\n",
            "Epoch 30/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4477 - accuracy: 0.8681 - val_loss: 0.4833 - val_accuracy: 0.8612\n",
            "Epoch 31/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4460 - accuracy: 0.8691 - val_loss: 0.6278 - val_accuracy: 0.8161\n",
            "Epoch 32/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4473 - accuracy: 0.8675 - val_loss: 0.4715 - val_accuracy: 0.8671\n",
            "Epoch 33/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4452 - accuracy: 0.8686 - val_loss: 0.4915 - val_accuracy: 0.8546\n",
            "Epoch 34/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4468 - accuracy: 0.8687 - val_loss: 0.4844 - val_accuracy: 0.8619\n",
            "Epoch 35/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4466 - accuracy: 0.8694 - val_loss: 0.4853 - val_accuracy: 0.8608\n",
            "Epoch 36/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4469 - accuracy: 0.8673 - val_loss: 0.4825 - val_accuracy: 0.8613\n",
            "Epoch 37/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4425 - accuracy: 0.8694 - val_loss: 0.4829 - val_accuracy: 0.8637\n",
            "Epoch 38/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4439 - accuracy: 0.8687 - val_loss: 0.4854 - val_accuracy: 0.8624\n",
            "Epoch 39/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4456 - accuracy: 0.8674 - val_loss: 0.4801 - val_accuracy: 0.8601\n",
            "Epoch 40/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4433 - accuracy: 0.8689 - val_loss: 0.4737 - val_accuracy: 0.8656\n",
            "Epoch 41/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4434 - accuracy: 0.8690 - val_loss: 0.5041 - val_accuracy: 0.8526\n",
            "Epoch 42/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4451 - accuracy: 0.8696 - val_loss: 0.4766 - val_accuracy: 0.8602\n",
            "Epoch 43/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4449 - accuracy: 0.8682 - val_loss: 0.5258 - val_accuracy: 0.8425\n",
            "Epoch 44/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4428 - accuracy: 0.8698 - val_loss: 0.4779 - val_accuracy: 0.8649\n",
            "Epoch 45/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4442 - accuracy: 0.8683 - val_loss: 0.5260 - val_accuracy: 0.8447\n",
            "Epoch 46/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4461 - accuracy: 0.8693 - val_loss: 0.4950 - val_accuracy: 0.8577\n",
            "Epoch 47/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4440 - accuracy: 0.8687 - val_loss: 0.5187 - val_accuracy: 0.8496\n",
            "Epoch 48/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4431 - accuracy: 0.8700 - val_loss: 0.4678 - val_accuracy: 0.8618\n",
            "Epoch 49/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4406 - accuracy: 0.8702 - val_loss: 0.4869 - val_accuracy: 0.8581\n",
            "Epoch 50/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4423 - accuracy: 0.8705 - val_loss: 0.4844 - val_accuracy: 0.8648\n",
            "Epoch 51/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4419 - accuracy: 0.8690 - val_loss: 0.5301 - val_accuracy: 0.8497\n",
            "Epoch 52/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4422 - accuracy: 0.8704 - val_loss: 0.5099 - val_accuracy: 0.8569\n",
            "Epoch 53/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4404 - accuracy: 0.8698 - val_loss: 0.4706 - val_accuracy: 0.8667\n",
            "Epoch 54/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4435 - accuracy: 0.8700 - val_loss: 0.5225 - val_accuracy: 0.8513\n",
            "Epoch 55/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4415 - accuracy: 0.8701 - val_loss: 0.4928 - val_accuracy: 0.8620\n",
            "Epoch 56/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4422 - accuracy: 0.8694 - val_loss: 0.5006 - val_accuracy: 0.8524\n",
            "Epoch 57/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4430 - accuracy: 0.8700 - val_loss: 0.4914 - val_accuracy: 0.8575\n",
            "Epoch 58/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4386 - accuracy: 0.8705 - val_loss: 0.5002 - val_accuracy: 0.8627\n",
            "Epoch 59/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4428 - accuracy: 0.8690 - val_loss: 0.5084 - val_accuracy: 0.8508\n",
            "Epoch 60/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4411 - accuracy: 0.8686 - val_loss: 0.5019 - val_accuracy: 0.8558\n",
            "Epoch 61/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4401 - accuracy: 0.8693 - val_loss: 0.4924 - val_accuracy: 0.8583\n",
            "Epoch 62/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4397 - accuracy: 0.8685 - val_loss: 0.4799 - val_accuracy: 0.8622\n",
            "Epoch 63/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4394 - accuracy: 0.8705 - val_loss: 0.5020 - val_accuracy: 0.8572\n",
            "Epoch 64/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4403 - accuracy: 0.8691 - val_loss: 0.4890 - val_accuracy: 0.8655\n",
            "Epoch 65/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4416 - accuracy: 0.8706 - val_loss: 0.5139 - val_accuracy: 0.8537\n",
            "Epoch 66/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4365 - accuracy: 0.8703 - val_loss: 0.4839 - val_accuracy: 0.8637\n",
            "Epoch 67/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4365 - accuracy: 0.8714 - val_loss: 0.4812 - val_accuracy: 0.8641\n",
            "Epoch 68/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4430 - accuracy: 0.8685 - val_loss: 0.4919 - val_accuracy: 0.8618\n",
            "Epoch 69/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4385 - accuracy: 0.8707 - val_loss: 0.5136 - val_accuracy: 0.8483\n",
            "Epoch 70/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4354 - accuracy: 0.8714 - val_loss: 0.4996 - val_accuracy: 0.8572\n",
            "Epoch 71/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4392 - accuracy: 0.8710 - val_loss: 0.4767 - val_accuracy: 0.8611\n",
            "Epoch 72/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4371 - accuracy: 0.8704 - val_loss: 0.4994 - val_accuracy: 0.8575\n",
            "Epoch 73/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4407 - accuracy: 0.8692 - val_loss: 0.4854 - val_accuracy: 0.8590\n",
            "Epoch 74/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4378 - accuracy: 0.8712 - val_loss: 0.4907 - val_accuracy: 0.8624\n",
            "Epoch 75/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4388 - accuracy: 0.8712 - val_loss: 0.4711 - val_accuracy: 0.8586\n",
            "Epoch 76/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4399 - accuracy: 0.8698 - val_loss: 0.5053 - val_accuracy: 0.8545\n",
            "Epoch 77/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4415 - accuracy: 0.8695 - val_loss: 0.4845 - val_accuracy: 0.8652\n",
            "Epoch 78/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4409 - accuracy: 0.8696 - val_loss: 0.4854 - val_accuracy: 0.8652\n",
            "Epoch 79/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4386 - accuracy: 0.8708 - val_loss: 0.5097 - val_accuracy: 0.8562\n",
            "Epoch 80/100\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4384 - accuracy: 0.8702 - val_loss: 0.4987 - val_accuracy: 0.8595\n",
            "Epoch 81/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4402 - accuracy: 0.8698 - val_loss: 0.4957 - val_accuracy: 0.8562\n",
            "Epoch 82/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4372 - accuracy: 0.8709 - val_loss: 0.4778 - val_accuracy: 0.8646\n",
            "Epoch 83/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4401 - accuracy: 0.8702 - val_loss: 0.4902 - val_accuracy: 0.8622\n",
            "Epoch 84/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4386 - accuracy: 0.8701 - val_loss: 0.5031 - val_accuracy: 0.8586\n",
            "Epoch 85/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4366 - accuracy: 0.8710 - val_loss: 0.4733 - val_accuracy: 0.8613\n",
            "Epoch 86/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4379 - accuracy: 0.8709 - val_loss: 0.4985 - val_accuracy: 0.8538\n",
            "Epoch 87/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4381 - accuracy: 0.8708 - val_loss: 0.4957 - val_accuracy: 0.8559\n",
            "Epoch 88/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4409 - accuracy: 0.8699 - val_loss: 0.4861 - val_accuracy: 0.8608\n",
            "Epoch 89/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4353 - accuracy: 0.8729 - val_loss: 0.5053 - val_accuracy: 0.8575\n",
            "Epoch 90/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4392 - accuracy: 0.8710 - val_loss: 0.4932 - val_accuracy: 0.8592\n",
            "Epoch 91/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4386 - accuracy: 0.8701 - val_loss: 0.4949 - val_accuracy: 0.8512\n",
            "Epoch 92/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4412 - accuracy: 0.8689 - val_loss: 0.5249 - val_accuracy: 0.8515\n",
            "Epoch 93/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4370 - accuracy: 0.8718 - val_loss: 0.4894 - val_accuracy: 0.8622\n",
            "Epoch 94/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4381 - accuracy: 0.8711 - val_loss: 0.4888 - val_accuracy: 0.8622\n",
            "Epoch 95/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4367 - accuracy: 0.8708 - val_loss: 0.5028 - val_accuracy: 0.8583\n",
            "Epoch 96/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4380 - accuracy: 0.8698 - val_loss: 0.4810 - val_accuracy: 0.8632\n",
            "Epoch 97/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4385 - accuracy: 0.8696 - val_loss: 0.5007 - val_accuracy: 0.8563\n",
            "Epoch 98/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4370 - accuracy: 0.8710 - val_loss: 0.5009 - val_accuracy: 0.8523\n",
            "Epoch 99/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4407 - accuracy: 0.8694 - val_loss: 0.4800 - val_accuracy: 0.8609\n",
            "Epoch 100/100\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4363 - accuracy: 0.8713 - val_loss: 0.4776 - val_accuracy: 0.8603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would we've trained our model pretty much with an accuracy around of **88%** now in Training samples.\n",
        "\n",
        "But was it because of setting learning rate only ? Not really. \n",
        "\n",
        "It may be because I let the model to train and learn the patterns for little longer i.e 100 epochs and I think let's increase it a little bit more as `200` epochs and see what happens.\n",
        "\n",
        "### Training the model little longer\n",
        "\n",
        "Let's increase the number of epochs and the plot the loss (or training) curves to know how did the performance change everytime the model had a change to look at the data (once every epoch) ?"
      ],
      "metadata": {
        "id": "-8Krus8OazRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# create a model\n",
        "model_5 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile a model\n",
        "model_5.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # default\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# fit the model\n",
        "history_5 = model_5.fit(X_train_norm,\n",
        "                        y_train,\n",
        "                        epochs=200, # This time for longer peroid of epochs\n",
        "                        validation_data = (X_valid_norm, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih6ogqfMeKKC",
        "outputId": "5155c31a-22bf-4e78-fe05-a3d3ab93f0a0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 1.4868 - accuracy: 0.4670 - val_loss: 0.9892 - val_accuracy: 0.7036\n",
            "Epoch 2/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8228 - accuracy: 0.7546 - val_loss: 0.7323 - val_accuracy: 0.7837\n",
            "Epoch 3/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7065 - accuracy: 0.7875 - val_loss: 0.6910 - val_accuracy: 0.7921\n",
            "Epoch 4/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6661 - accuracy: 0.8008 - val_loss: 0.6584 - val_accuracy: 0.8046\n",
            "Epoch 5/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6395 - accuracy: 0.8078 - val_loss: 0.6314 - val_accuracy: 0.8148\n",
            "Epoch 6/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6170 - accuracy: 0.8138 - val_loss: 0.6209 - val_accuracy: 0.8179\n",
            "Epoch 7/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5966 - accuracy: 0.8207 - val_loss: 0.5997 - val_accuracy: 0.8263\n",
            "Epoch 8/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5748 - accuracy: 0.8284 - val_loss: 0.5784 - val_accuracy: 0.8324\n",
            "Epoch 9/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5508 - accuracy: 0.8375 - val_loss: 0.5514 - val_accuracy: 0.8428\n",
            "Epoch 10/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.8447 - val_loss: 0.5572 - val_accuracy: 0.8393\n",
            "Epoch 11/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5171 - accuracy: 0.8501 - val_loss: 0.5492 - val_accuracy: 0.8397\n",
            "Epoch 12/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5050 - accuracy: 0.8527 - val_loss: 0.5250 - val_accuracy: 0.8535\n",
            "Epoch 13/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4960 - accuracy: 0.8553 - val_loss: 0.5109 - val_accuracy: 0.8581\n",
            "Epoch 14/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4881 - accuracy: 0.8581 - val_loss: 0.5120 - val_accuracy: 0.8520\n",
            "Epoch 15/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4796 - accuracy: 0.8612 - val_loss: 0.5043 - val_accuracy: 0.8590\n",
            "Epoch 16/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4741 - accuracy: 0.8629 - val_loss: 0.4922 - val_accuracy: 0.8635\n",
            "Epoch 17/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4680 - accuracy: 0.8660 - val_loss: 0.4884 - val_accuracy: 0.8635\n",
            "Epoch 18/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4628 - accuracy: 0.8668 - val_loss: 0.4912 - val_accuracy: 0.8633\n",
            "Epoch 19/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4587 - accuracy: 0.8689 - val_loss: 0.4883 - val_accuracy: 0.8601\n",
            "Epoch 20/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4544 - accuracy: 0.8694 - val_loss: 0.4919 - val_accuracy: 0.8602\n",
            "Epoch 21/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4510 - accuracy: 0.8705 - val_loss: 0.4838 - val_accuracy: 0.8624\n",
            "Epoch 22/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4468 - accuracy: 0.8706 - val_loss: 0.4752 - val_accuracy: 0.8643\n",
            "Epoch 23/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4442 - accuracy: 0.8720 - val_loss: 0.4766 - val_accuracy: 0.8658\n",
            "Epoch 24/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4410 - accuracy: 0.8735 - val_loss: 0.4690 - val_accuracy: 0.8694\n",
            "Epoch 25/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4393 - accuracy: 0.8737 - val_loss: 0.4709 - val_accuracy: 0.8674\n",
            "Epoch 26/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4372 - accuracy: 0.8744 - val_loss: 0.4639 - val_accuracy: 0.8704\n",
            "Epoch 27/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4349 - accuracy: 0.8748 - val_loss: 0.4686 - val_accuracy: 0.8697\n",
            "Epoch 28/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4332 - accuracy: 0.8759 - val_loss: 0.4729 - val_accuracy: 0.8675\n",
            "Epoch 29/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4324 - accuracy: 0.8759 - val_loss: 0.4620 - val_accuracy: 0.8705\n",
            "Epoch 30/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4304 - accuracy: 0.8765 - val_loss: 0.4629 - val_accuracy: 0.8723\n",
            "Epoch 31/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4292 - accuracy: 0.8768 - val_loss: 0.4576 - val_accuracy: 0.8720\n",
            "Epoch 32/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4275 - accuracy: 0.8773 - val_loss: 0.4570 - val_accuracy: 0.8710\n",
            "Epoch 33/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4261 - accuracy: 0.8766 - val_loss: 0.4585 - val_accuracy: 0.8705\n",
            "Epoch 34/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4249 - accuracy: 0.8790 - val_loss: 0.4606 - val_accuracy: 0.8718\n",
            "Epoch 35/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4246 - accuracy: 0.8791 - val_loss: 0.4634 - val_accuracy: 0.8726\n",
            "Epoch 36/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4227 - accuracy: 0.8780 - val_loss: 0.4539 - val_accuracy: 0.8733\n",
            "Epoch 37/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4224 - accuracy: 0.8784 - val_loss: 0.4632 - val_accuracy: 0.8703\n",
            "Epoch 38/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4207 - accuracy: 0.8784 - val_loss: 0.4572 - val_accuracy: 0.8732\n",
            "Epoch 39/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4201 - accuracy: 0.8803 - val_loss: 0.4599 - val_accuracy: 0.8697\n",
            "Epoch 40/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4192 - accuracy: 0.8790 - val_loss: 0.4573 - val_accuracy: 0.8730\n",
            "Epoch 41/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4189 - accuracy: 0.8788 - val_loss: 0.4582 - val_accuracy: 0.8761\n",
            "Epoch 42/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4179 - accuracy: 0.8804 - val_loss: 0.4575 - val_accuracy: 0.8727\n",
            "Epoch 43/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4162 - accuracy: 0.8797 - val_loss: 0.4546 - val_accuracy: 0.8742\n",
            "Epoch 44/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4161 - accuracy: 0.8808 - val_loss: 0.4575 - val_accuracy: 0.8737\n",
            "Epoch 45/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4155 - accuracy: 0.8799 - val_loss: 0.4661 - val_accuracy: 0.8698\n",
            "Epoch 46/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4149 - accuracy: 0.8802 - val_loss: 0.4601 - val_accuracy: 0.8729\n",
            "Epoch 47/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4139 - accuracy: 0.8803 - val_loss: 0.4615 - val_accuracy: 0.8734\n",
            "Epoch 48/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4126 - accuracy: 0.8816 - val_loss: 0.4527 - val_accuracy: 0.8751\n",
            "Epoch 49/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4129 - accuracy: 0.8805 - val_loss: 0.4581 - val_accuracy: 0.8721\n",
            "Epoch 50/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4126 - accuracy: 0.8813 - val_loss: 0.4515 - val_accuracy: 0.8762\n",
            "Epoch 51/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4116 - accuracy: 0.8814 - val_loss: 0.4585 - val_accuracy: 0.8736\n",
            "Epoch 52/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4107 - accuracy: 0.8816 - val_loss: 0.4535 - val_accuracy: 0.8752\n",
            "Epoch 53/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4100 - accuracy: 0.8813 - val_loss: 0.4631 - val_accuracy: 0.8720\n",
            "Epoch 54/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4104 - accuracy: 0.8817 - val_loss: 0.4544 - val_accuracy: 0.8742\n",
            "Epoch 55/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4094 - accuracy: 0.8831 - val_loss: 0.4601 - val_accuracy: 0.8733\n",
            "Epoch 56/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4084 - accuracy: 0.8815 - val_loss: 0.4617 - val_accuracy: 0.8700\n",
            "Epoch 57/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4085 - accuracy: 0.8823 - val_loss: 0.4543 - val_accuracy: 0.8748\n",
            "Epoch 58/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4079 - accuracy: 0.8829 - val_loss: 0.4541 - val_accuracy: 0.8748\n",
            "Epoch 59/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4080 - accuracy: 0.8830 - val_loss: 0.4495 - val_accuracy: 0.8754\n",
            "Epoch 60/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4074 - accuracy: 0.8825 - val_loss: 0.4483 - val_accuracy: 0.8761\n",
            "Epoch 61/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4075 - accuracy: 0.8831 - val_loss: 0.4524 - val_accuracy: 0.8753\n",
            "Epoch 62/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4058 - accuracy: 0.8830 - val_loss: 0.4560 - val_accuracy: 0.8729\n",
            "Epoch 63/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4058 - accuracy: 0.8833 - val_loss: 0.4704 - val_accuracy: 0.8684\n",
            "Epoch 64/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4060 - accuracy: 0.8835 - val_loss: 0.4485 - val_accuracy: 0.8769\n",
            "Epoch 65/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4047 - accuracy: 0.8838 - val_loss: 0.4496 - val_accuracy: 0.8778\n",
            "Epoch 66/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4049 - accuracy: 0.8838 - val_loss: 0.4517 - val_accuracy: 0.8733\n",
            "Epoch 67/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4044 - accuracy: 0.8832 - val_loss: 0.4455 - val_accuracy: 0.8756\n",
            "Epoch 68/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4044 - accuracy: 0.8835 - val_loss: 0.4489 - val_accuracy: 0.8743\n",
            "Epoch 69/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4040 - accuracy: 0.8839 - val_loss: 0.4536 - val_accuracy: 0.8733\n",
            "Epoch 70/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4027 - accuracy: 0.8845 - val_loss: 0.4546 - val_accuracy: 0.8767\n",
            "Epoch 71/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4033 - accuracy: 0.8840 - val_loss: 0.4507 - val_accuracy: 0.8731\n",
            "Epoch 72/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4021 - accuracy: 0.8854 - val_loss: 0.4447 - val_accuracy: 0.8759\n",
            "Epoch 73/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4029 - accuracy: 0.8834 - val_loss: 0.4447 - val_accuracy: 0.8769\n",
            "Epoch 74/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4022 - accuracy: 0.8844 - val_loss: 0.4454 - val_accuracy: 0.8764\n",
            "Epoch 75/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4025 - accuracy: 0.8852 - val_loss: 0.4474 - val_accuracy: 0.8755\n",
            "Epoch 76/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4026 - accuracy: 0.8843 - val_loss: 0.4491 - val_accuracy: 0.8737\n",
            "Epoch 77/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4024 - accuracy: 0.8849 - val_loss: 0.4481 - val_accuracy: 0.8755\n",
            "Epoch 78/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4010 - accuracy: 0.8851 - val_loss: 0.4480 - val_accuracy: 0.8758\n",
            "Epoch 79/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4010 - accuracy: 0.8842 - val_loss: 0.4527 - val_accuracy: 0.8741\n",
            "Epoch 80/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4009 - accuracy: 0.8843 - val_loss: 0.4478 - val_accuracy: 0.8764\n",
            "Epoch 81/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4005 - accuracy: 0.8848 - val_loss: 0.4470 - val_accuracy: 0.8752\n",
            "Epoch 82/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4001 - accuracy: 0.8851 - val_loss: 0.4546 - val_accuracy: 0.8728\n",
            "Epoch 83/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3999 - accuracy: 0.8847 - val_loss: 0.4520 - val_accuracy: 0.8761\n",
            "Epoch 84/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3999 - accuracy: 0.8847 - val_loss: 0.4585 - val_accuracy: 0.8733\n",
            "Epoch 85/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8858 - val_loss: 0.4499 - val_accuracy: 0.8744\n",
            "Epoch 86/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4000 - accuracy: 0.8847 - val_loss: 0.4521 - val_accuracy: 0.8733\n",
            "Epoch 87/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3987 - accuracy: 0.8856 - val_loss: 0.4466 - val_accuracy: 0.8761\n",
            "Epoch 88/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3981 - accuracy: 0.8861 - val_loss: 0.4570 - val_accuracy: 0.8730\n",
            "Epoch 89/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3986 - accuracy: 0.8852 - val_loss: 0.4470 - val_accuracy: 0.8767\n",
            "Epoch 90/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3987 - accuracy: 0.8847 - val_loss: 0.4483 - val_accuracy: 0.8756\n",
            "Epoch 91/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3984 - accuracy: 0.8845 - val_loss: 0.4457 - val_accuracy: 0.8748\n",
            "Epoch 92/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3978 - accuracy: 0.8847 - val_loss: 0.4462 - val_accuracy: 0.8783\n",
            "Epoch 93/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3980 - accuracy: 0.8855 - val_loss: 0.4517 - val_accuracy: 0.8764\n",
            "Epoch 94/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3980 - accuracy: 0.8851 - val_loss: 0.4503 - val_accuracy: 0.8758\n",
            "Epoch 95/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3984 - accuracy: 0.8846 - val_loss: 0.4519 - val_accuracy: 0.8745\n",
            "Epoch 96/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3982 - accuracy: 0.8861 - val_loss: 0.4467 - val_accuracy: 0.8752\n",
            "Epoch 97/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3975 - accuracy: 0.8859 - val_loss: 0.4500 - val_accuracy: 0.8743\n",
            "Epoch 98/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3967 - accuracy: 0.8857 - val_loss: 0.4432 - val_accuracy: 0.8755\n",
            "Epoch 99/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3968 - accuracy: 0.8860 - val_loss: 0.4452 - val_accuracy: 0.8763\n",
            "Epoch 100/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3964 - accuracy: 0.8866 - val_loss: 0.4529 - val_accuracy: 0.8710\n",
            "Epoch 101/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3960 - accuracy: 0.8849 - val_loss: 0.4448 - val_accuracy: 0.8762\n",
            "Epoch 102/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3960 - accuracy: 0.8857 - val_loss: 0.4454 - val_accuracy: 0.8758\n",
            "Epoch 103/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3970 - accuracy: 0.8850 - val_loss: 0.4551 - val_accuracy: 0.8723\n",
            "Epoch 104/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3960 - accuracy: 0.8857 - val_loss: 0.4521 - val_accuracy: 0.8720\n",
            "Epoch 105/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3965 - accuracy: 0.8854 - val_loss: 0.4460 - val_accuracy: 0.8756\n",
            "Epoch 106/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3953 - accuracy: 0.8861 - val_loss: 0.4466 - val_accuracy: 0.8749\n",
            "Epoch 107/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3958 - accuracy: 0.8867 - val_loss: 0.4462 - val_accuracy: 0.8776\n",
            "Epoch 108/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3957 - accuracy: 0.8869 - val_loss: 0.4559 - val_accuracy: 0.8735\n",
            "Epoch 109/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3950 - accuracy: 0.8851 - val_loss: 0.4445 - val_accuracy: 0.8759\n",
            "Epoch 110/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3959 - accuracy: 0.8855 - val_loss: 0.4427 - val_accuracy: 0.8769\n",
            "Epoch 111/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3960 - accuracy: 0.8853 - val_loss: 0.4439 - val_accuracy: 0.8774\n",
            "Epoch 112/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3958 - accuracy: 0.8850 - val_loss: 0.4440 - val_accuracy: 0.8747\n",
            "Epoch 113/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3945 - accuracy: 0.8866 - val_loss: 0.4475 - val_accuracy: 0.8755\n",
            "Epoch 114/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3953 - accuracy: 0.8854 - val_loss: 0.4446 - val_accuracy: 0.8750\n",
            "Epoch 115/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3951 - accuracy: 0.8866 - val_loss: 0.4440 - val_accuracy: 0.8752\n",
            "Epoch 116/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3942 - accuracy: 0.8861 - val_loss: 0.4470 - val_accuracy: 0.8763\n",
            "Epoch 117/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3948 - accuracy: 0.8862 - val_loss: 0.4473 - val_accuracy: 0.8754\n",
            "Epoch 118/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3937 - accuracy: 0.8866 - val_loss: 0.4469 - val_accuracy: 0.8751\n",
            "Epoch 119/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3944 - accuracy: 0.8850 - val_loss: 0.4476 - val_accuracy: 0.8746\n",
            "Epoch 120/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3936 - accuracy: 0.8862 - val_loss: 0.4472 - val_accuracy: 0.8756\n",
            "Epoch 121/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3939 - accuracy: 0.8866 - val_loss: 0.4434 - val_accuracy: 0.8756\n",
            "Epoch 122/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3937 - accuracy: 0.8858 - val_loss: 0.4453 - val_accuracy: 0.8758\n",
            "Epoch 123/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3930 - accuracy: 0.8868 - val_loss: 0.4458 - val_accuracy: 0.8756\n",
            "Epoch 124/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3934 - accuracy: 0.8860 - val_loss: 0.4447 - val_accuracy: 0.8749\n",
            "Epoch 125/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3945 - accuracy: 0.8856 - val_loss: 0.4462 - val_accuracy: 0.8766\n",
            "Epoch 126/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3933 - accuracy: 0.8854 - val_loss: 0.4499 - val_accuracy: 0.8725\n",
            "Epoch 127/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3931 - accuracy: 0.8862 - val_loss: 0.4447 - val_accuracy: 0.8763\n",
            "Epoch 128/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3936 - accuracy: 0.8862 - val_loss: 0.4474 - val_accuracy: 0.8769\n",
            "Epoch 129/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3929 - accuracy: 0.8867 - val_loss: 0.4480 - val_accuracy: 0.8748\n",
            "Epoch 130/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3933 - accuracy: 0.8860 - val_loss: 0.4448 - val_accuracy: 0.8747\n",
            "Epoch 131/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3917 - accuracy: 0.8867 - val_loss: 0.4491 - val_accuracy: 0.8752\n",
            "Epoch 132/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3924 - accuracy: 0.8862 - val_loss: 0.4479 - val_accuracy: 0.8723\n",
            "Epoch 133/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3939 - accuracy: 0.8865 - val_loss: 0.4442 - val_accuracy: 0.8769\n",
            "Epoch 134/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3923 - accuracy: 0.8871 - val_loss: 0.4441 - val_accuracy: 0.8761\n",
            "Epoch 135/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3924 - accuracy: 0.8867 - val_loss: 0.4495 - val_accuracy: 0.8744\n",
            "Epoch 136/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3915 - accuracy: 0.8867 - val_loss: 0.4510 - val_accuracy: 0.8742\n",
            "Epoch 137/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3918 - accuracy: 0.8866 - val_loss: 0.4475 - val_accuracy: 0.8747\n",
            "Epoch 138/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3922 - accuracy: 0.8863 - val_loss: 0.4437 - val_accuracy: 0.8783\n",
            "Epoch 139/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3912 - accuracy: 0.8871 - val_loss: 0.4508 - val_accuracy: 0.8758\n",
            "Epoch 140/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3907 - accuracy: 0.8864 - val_loss: 0.4447 - val_accuracy: 0.8760\n",
            "Epoch 141/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3907 - accuracy: 0.8877 - val_loss: 0.4560 - val_accuracy: 0.8724\n",
            "Epoch 142/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3911 - accuracy: 0.8873 - val_loss: 0.4465 - val_accuracy: 0.8746\n",
            "Epoch 143/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3913 - accuracy: 0.8866 - val_loss: 0.4530 - val_accuracy: 0.8742\n",
            "Epoch 144/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3915 - accuracy: 0.8861 - val_loss: 0.4466 - val_accuracy: 0.8739\n",
            "Epoch 145/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3900 - accuracy: 0.8868 - val_loss: 0.4536 - val_accuracy: 0.8743\n",
            "Epoch 146/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3908 - accuracy: 0.8863 - val_loss: 0.4437 - val_accuracy: 0.8753\n",
            "Epoch 147/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3893 - accuracy: 0.8883 - val_loss: 0.4487 - val_accuracy: 0.8743\n",
            "Epoch 148/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3898 - accuracy: 0.8877 - val_loss: 0.4455 - val_accuracy: 0.8742\n",
            "Epoch 149/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3911 - accuracy: 0.8868 - val_loss: 0.4491 - val_accuracy: 0.8751\n",
            "Epoch 150/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3897 - accuracy: 0.8875 - val_loss: 0.4465 - val_accuracy: 0.8733\n",
            "Epoch 151/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3901 - accuracy: 0.8874 - val_loss: 0.4427 - val_accuracy: 0.8756\n",
            "Epoch 152/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3898 - accuracy: 0.8877 - val_loss: 0.4433 - val_accuracy: 0.8763\n",
            "Epoch 153/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3893 - accuracy: 0.8877 - val_loss: 0.4432 - val_accuracy: 0.8758\n",
            "Epoch 154/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3898 - accuracy: 0.8873 - val_loss: 0.4454 - val_accuracy: 0.8736\n",
            "Epoch 155/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3895 - accuracy: 0.8859 - val_loss: 0.4464 - val_accuracy: 0.8776\n",
            "Epoch 156/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3894 - accuracy: 0.8879 - val_loss: 0.4432 - val_accuracy: 0.8778\n",
            "Epoch 157/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3886 - accuracy: 0.8875 - val_loss: 0.4488 - val_accuracy: 0.8744\n",
            "Epoch 158/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3896 - accuracy: 0.8866 - val_loss: 0.4425 - val_accuracy: 0.8737\n",
            "Epoch 159/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3883 - accuracy: 0.8872 - val_loss: 0.4449 - val_accuracy: 0.8763\n",
            "Epoch 160/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3888 - accuracy: 0.8866 - val_loss: 0.4636 - val_accuracy: 0.8692\n",
            "Epoch 161/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3883 - accuracy: 0.8878 - val_loss: 0.4442 - val_accuracy: 0.8763\n",
            "Epoch 162/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3883 - accuracy: 0.8875 - val_loss: 0.4445 - val_accuracy: 0.8748\n",
            "Epoch 163/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3887 - accuracy: 0.8866 - val_loss: 0.4413 - val_accuracy: 0.8745\n",
            "Epoch 164/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3891 - accuracy: 0.8867 - val_loss: 0.4428 - val_accuracy: 0.8754\n",
            "Epoch 165/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3887 - accuracy: 0.8867 - val_loss: 0.4400 - val_accuracy: 0.8775\n",
            "Epoch 166/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3887 - accuracy: 0.8866 - val_loss: 0.4518 - val_accuracy: 0.8742\n",
            "Epoch 167/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3876 - accuracy: 0.8877 - val_loss: 0.4539 - val_accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3875 - accuracy: 0.8868 - val_loss: 0.4490 - val_accuracy: 0.8742\n",
            "Epoch 169/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3880 - accuracy: 0.8861 - val_loss: 0.4457 - val_accuracy: 0.8770\n",
            "Epoch 170/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3874 - accuracy: 0.8874 - val_loss: 0.4415 - val_accuracy: 0.8752\n",
            "Epoch 171/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3880 - accuracy: 0.8864 - val_loss: 0.4471 - val_accuracy: 0.8773\n",
            "Epoch 172/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3873 - accuracy: 0.8875 - val_loss: 0.4480 - val_accuracy: 0.8767\n",
            "Epoch 173/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3868 - accuracy: 0.8876 - val_loss: 0.4545 - val_accuracy: 0.8730\n",
            "Epoch 174/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3871 - accuracy: 0.8869 - val_loss: 0.4454 - val_accuracy: 0.8755\n",
            "Epoch 175/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3868 - accuracy: 0.8871 - val_loss: 0.4431 - val_accuracy: 0.8774\n",
            "Epoch 176/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3867 - accuracy: 0.8870 - val_loss: 0.4458 - val_accuracy: 0.8758\n",
            "Epoch 177/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3874 - accuracy: 0.8871 - val_loss: 0.4415 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3866 - accuracy: 0.8873 - val_loss: 0.4441 - val_accuracy: 0.8758\n",
            "Epoch 179/200\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3871 - accuracy: 0.8875 - val_loss: 0.4464 - val_accuracy: 0.8729\n",
            "Epoch 180/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3864 - accuracy: 0.8870 - val_loss: 0.4431 - val_accuracy: 0.8737\n",
            "Epoch 181/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3857 - accuracy: 0.8874 - val_loss: 0.4518 - val_accuracy: 0.8740\n",
            "Epoch 182/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3865 - accuracy: 0.8861 - val_loss: 0.4405 - val_accuracy: 0.8761\n",
            "Epoch 183/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3860 - accuracy: 0.8869 - val_loss: 0.4428 - val_accuracy: 0.8743\n",
            "Epoch 184/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3859 - accuracy: 0.8865 - val_loss: 0.4480 - val_accuracy: 0.8752\n",
            "Epoch 185/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3863 - accuracy: 0.8873 - val_loss: 0.4414 - val_accuracy: 0.8767\n",
            "Epoch 186/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3854 - accuracy: 0.8870 - val_loss: 0.4418 - val_accuracy: 0.8767\n",
            "Epoch 187/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3853 - accuracy: 0.8874 - val_loss: 0.4452 - val_accuracy: 0.8766\n",
            "Epoch 188/200\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3866 - accuracy: 0.8864 - val_loss: 0.4418 - val_accuracy: 0.8759\n",
            "Epoch 189/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3861 - accuracy: 0.8877 - val_loss: 0.4414 - val_accuracy: 0.8755\n",
            "Epoch 190/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3856 - accuracy: 0.8874 - val_loss: 0.4490 - val_accuracy: 0.8736\n",
            "Epoch 191/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3858 - accuracy: 0.8865 - val_loss: 0.4476 - val_accuracy: 0.8758\n",
            "Epoch 192/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3851 - accuracy: 0.8880 - val_loss: 0.4453 - val_accuracy: 0.8761\n",
            "Epoch 193/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3855 - accuracy: 0.8873 - val_loss: 0.4505 - val_accuracy: 0.8751\n",
            "Epoch 194/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3860 - accuracy: 0.8864 - val_loss: 0.4488 - val_accuracy: 0.8734\n",
            "Epoch 195/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3856 - accuracy: 0.8867 - val_loss: 0.4470 - val_accuracy: 0.8737\n",
            "Epoch 196/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3860 - accuracy: 0.8871 - val_loss: 0.4419 - val_accuracy: 0.8757\n",
            "Epoch 197/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3851 - accuracy: 0.8871 - val_loss: 0.4459 - val_accuracy: 0.8747\n",
            "Epoch 198/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3850 - accuracy: 0.8866 - val_loss: 0.4532 - val_accuracy: 0.8728\n",
            "Epoch 199/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3854 - accuracy: 0.8877 - val_loss: 0.4478 - val_accuracy: 0.8759\n",
            "Epoch 200/200\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3852 - accuracy: 0.8874 - val_loss: 0.4428 - val_accuracy: 0.8764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems no difference at all.\n",
        "\n",
        "How about plotting the loss curve."
      ],
      "metadata": {
        "id": "oZ30nar5es6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history_5.history)"
      ],
      "metadata": {
        "id": "anDvCBH4i3Fi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(17, 7))\n",
        "plt.plot(history_df['loss'])\n",
        "plt.plot(history_df['accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Title');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "pmw384nmi8gh",
        "outputId": "b88f485b-880b-4521-f738-8a1eade1d1d0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1224x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like even we increase the number of epochs the loss will decrease constantly for longer period of time but it'll not drastically decrease upto 0 and increase the accuracy to upto `99%`. May be we can find another way to tune our model and get the higher accuracy."
      ],
      "metadata": {
        "id": "eJqgPpYXk9bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tweak with Neural Network layers and it's units."
      ],
      "metadata": {
        "id": "4QFZz1_9mMBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusting Neural Network layers and neurons\n",
        "\n",
        "We can add more layers and increase the neurons in each layer."
      ],
      "metadata": {
        "id": "NEUWYn5HmRhL"
      }
    }
  ]
}